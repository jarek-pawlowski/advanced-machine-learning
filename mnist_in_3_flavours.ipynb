{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist_in_3_flavours.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNzi4mqRG8xwBOFACyt4o46",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jarek-pawlowski/advanced-machine-learning/blob/main/mnist_in_3_flavours.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BredVsAxhjoz"
      },
      "source": [
        "# Classification of handwritten digits using various neural network models with PyTorch\n",
        "* Multinomial classification task of images of handwritten digits (10 classes).\n",
        "* The database has a training set of 60k examples, and a test set of 10k examples, each image is of 28x28 pixels size.\n",
        "* The MNIST (\"Modified National Institute of Standards and Technology\") is the “hello world” dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsDMQq10yHkG"
      },
      "source": [
        "Before we start we need to load libraries and setup the model parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grvT221cyShc"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model_args = {}\n",
        "# random seed\n",
        "model_args['seed'] = 123\n",
        "# we will use batch size of 128 in Stochastic Gradient Descent (SGD) optimization of the network\n",
        "model_args['batch_size'] = 128 \n",
        "# learning rate is how fast it will decend\n",
        "model_args['lr'] = .01\n",
        "# SGD momentum (default: .5) momentum is a moving average of gradients (it helps to keep direction) \n",
        "model_args['momentum'] = .5\n",
        "# the number of epochs is the number of times you go through the full dataset \n",
        "model_args['epochs'] = 10\n",
        "# logging frequency\n",
        "model_args['log_interval'] = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMMMbtzvpYgR"
      },
      "source": [
        "Let's start with loading the dataset and check how it looks like"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4mwzWUvpmLq"
      },
      "source": [
        "# load the MINST dataset \n",
        "transform=transforms.Compose([transforms.ToTensor(), \n",
        "                              transforms.Normalize((0.1307,), (0.3081,))])\n",
        "mnist_train = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
        "# we divide this data into training and validation subsets\n",
        "train_subset, validation_subset = torch.utils.data.random_split(mnist_train, [50000, 10000])\n",
        "test_subset = datasets.MNIST('../data', train=False, download=True, transform=transform)\n",
        "\n",
        "# define dataloaders\n",
        "loader_kwargs = {'batch_size': model_args['batch_size'], \n",
        "                 'num_workers': 1, \n",
        "                 'pin_memory': True, \n",
        "                 'shuffle': True}\n",
        "train_loader = torch.utils.data.DataLoader(train_subset, **loader_kwargs)\n",
        "validation_loader = torch.utils.data.DataLoader(validation_subset, **loader_kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(test_subset, **loader_kwargs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Bhqwulj1nfy"
      },
      "source": [
        "print(len(train_subset))\n",
        "print(len(validation_subset))\n",
        "print(len(test_subset)) # we have data already divided into train & validation & test subsets\n",
        "print(len(train_loader)) # no of train batches\n",
        "example_number = 123\n",
        "print(train_subset[example_number][0][0].shape) # single item shape\n",
        "\n",
        "fig, axs = plt.subplots(5, 5, figsize=(7,7), tight_layout=True)\n",
        "for i in range(5):\n",
        "    for j in range(5):\n",
        "        axs[i,j].imshow(train_subset[example_number+i*5+j][0].reshape(28,28), cmap='gray')\n",
        "        axs[i,j].set_title(train_subset[example_number+i*5+j][1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBkkgV5G7Bvb"
      },
      "source": [
        "Let's now construct the NN models:\n",
        "- we start with siplest single layer **perceptron** with *softmax* activation,\n",
        "- then test *deep* network with some (fully-connected) *hidden* layers,\n",
        "- and finally setup deep **convolutional neural network**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6W19lU8aLvuG"
      },
      "source": [
        "class Perceptron(nn.Module):\n",
        "    # this defines the structure of the Perceptron model\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        # convolutional layer with 10 kernels of size 5x5\n",
        "        self.conv1 = nn.Conv2d(1, 5, kernel_size=5) \n",
        "        # 10 kernels of size 5x5\n",
        "        self.conv2 = nn.Conv2d(5, 10, kernel_size=5)\n",
        "        # 2D dropout\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        # fully connected layers\n",
        "        self.fc1 = nn.Linear(160, 20)\n",
        "        self.fc2 = nn.Linear(20, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1st layers group\n",
        "        x = self.conv1(x) # resulting in 5 feature maps each of size 24x24\n",
        "        x = F.max_pool2d(x, 2) # downsizing each map to 12x12\n",
        "        x = F.relu(x) # standard (in CNNs) ReLU activation\n",
        "        # 2nd group\n",
        "        x = self.conv2(x) # resulting in 10 feature maps each of size 8x8\n",
        "        x = self.conv2_drop(x) # dropout is a type of regularization\n",
        "        x = F.max_pool2d(x, 2) # downsizing each map to 4x4\n",
        "        x = F.relu(x)\n",
        "        # fully connected layers\n",
        "        x = x.view(-1, 160) # 10 maps of 4x4 size gives 160 numbers\n",
        "        x = self.fc1(x) # 160 -> 20\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x) # 20 -> 10\n",
        "        # softmax (multinomial classification) gives probabilities of each class\n",
        "        return F.log_softmax(x, dim=1) # note that dim=0 is the number of samples in batch\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    # this defines the structure of the CNN model\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        # convolutional layer with 10 kernels of size 5x5\n",
        "        self.conv1 = nn.Conv2d(1, 5, kernel_size=5) \n",
        "        # 10 kernels of size 5x5\n",
        "        self.conv2 = nn.Conv2d(5, 10, kernel_size=5)\n",
        "        # 2D dropout\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        # fully connected layers\n",
        "        self.fc1 = nn.Linear(160, 20)\n",
        "        self.fc2 = nn.Linear(20, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1st layers group\n",
        "        x = self.conv1(x) # resulting in 5 feature maps each of size 24x24\n",
        "        x = F.max_pool2d(x, 2) # downsizing each map to 12x12\n",
        "        x = F.relu(x) # standard (in CNNs) ReLU activation\n",
        "        # 2nd group\n",
        "        x = self.conv2(x) # resulting in 10 feature maps each of size 8x8\n",
        "        # x = self.conv2_drop(x) \n",
        "        x = F.max_pool2d(x, 2) # downsizing each map to 4x4\n",
        "        x = F.relu(x)\n",
        "        # fully connected layers\n",
        "        x = x.view(-1, 160) # 10 maps of 4x4 size gives 160 numbers\n",
        "        x = self.fc1(x) # 160 -> 20\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, training=self.training) # dropout is a type of regularization\n",
        "        x = self.fc2(x) # 20 -> 10\n",
        "        # softmax (multinomial classification) gives probabilities of each class\n",
        "        return F.log_softmax(x, dim=1) # note that dim=0 is the number of samples in batch\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch_number):\n",
        "    model.train()\n",
        "    train_loss = 0.\n",
        "    # get subsequent batches over the data in a given epoch\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        # send data tensors to GPU (or CPU)\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        # this will zero out the gradients for this batch\n",
        "        optimizer.zero_grad()\n",
        "        # this will execute the forward() function\n",
        "        output = model(data)\n",
        "        # calculate the negative-log-likelihood loss\n",
        "        loss = F.nll_loss(output, target, reduction='mean')\n",
        "        # backpropagate the loss\n",
        "        loss.backward()\n",
        "        # update the model weights (with assumed learning rate)\n",
        "        optimizer.step()\n",
        "        if batch_idx % model_args['log_interval'] == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch_number, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "        train_loss += loss.item()\n",
        "        \n",
        "    train_loss /= len(train_loader)\n",
        "    print('\\nTrain set: Average loss: {:.4f}'.format(train_loss))\n",
        "    \n",
        "def test(model, device, test_loader, message):\n",
        "    model.eval()\n",
        "    test_loss = 0.\n",
        "    correct = 0\n",
        "    # this is just inference, we don't need to calculate gradients\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device) \n",
        "            output = model(data)\n",
        "            # calculate and sum up batch loss\n",
        "            test_loss += F.nll_loss(output, target, reduction='mean') \n",
        "            # get the index of class with the max log-probability \n",
        "            prediction = output.argmax(dim=1)  \n",
        "            # item() returns value of the given tensor\n",
        "            correct += prediction.eq(target).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    print('{}: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        message, test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzzZ2T5FmqYe"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = CNN().to(device)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=model_args['lr'], momentum=model_args['momentum'])\n",
        "#optimizer = optim.Adadelta(model.parameters(), lr=model_args['lr'])\n",
        "\n",
        "torch.manual_seed(model_args['seed'])\n",
        "for epoch_number in range(1, model_args['epochs'] + 1):\n",
        "    train(model, device, train_loader, optimizer, epoch_number)\n",
        "    test(model, device, validation_loader, 'Validation set')\n",
        "\n",
        "test(model, device, test_loader, 'Test set')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mL2IBnmiZgr-"
      },
      "source": [
        "# Tasks to do\n",
        "* apply some regularization technique to *Deep* model in order to avoid overfitting,\n",
        "* try to figure out why the validation loss for *CNN* model turns to be *lower* than the train loss (hint: regularization???),\n",
        "* **tune one of these models to get the *Test Set Accuracy* > 99%**,\n",
        "* plot the *confusion matrix* among all of the classes---which of digits are most often confused with each other?,\n",
        "* and finally prepare report.\n",
        "\n",
        "# How the report should look like\n",
        "* 1-2 page long\n",
        "* shortly describe what modifications you apply\n",
        "* justyfy in a few words why your improved model is so cool\n",
        "* results should be presented as plots with multiline captions"
      ]
    }
  ]
}