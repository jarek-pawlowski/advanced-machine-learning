{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist_in_3_flavours.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMoBwHcUUG5a91E9krp76R6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jarek-pawlowski/advanced-machine-learning/blob/main/mnist_in_3_flavours.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BredVsAxhjoz"
      },
      "source": [
        "# Classification of handwritten digits using various neural network models with PyTorch\n",
        "* Multinomial classification task of images of handwritten digits (10 classes).\n",
        "* The database has a training set of 60k examples, and a test set of 10k examples, each image is of 28x28 pixels size.\n",
        "* The MNIST (\"Modified National Institute of Standards and Technology\") is the “hello world” dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEVy0c9ksHAU"
      },
      "source": [
        "## Model setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsDMQq10yHkG"
      },
      "source": [
        "Before we start we need to load libraries and setup the model parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grvT221cyShc"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams.update({'font.size': 14})\n",
        "\n",
        "model_args = {}\n",
        "# random seed\n",
        "model_args['seed'] = 123\n",
        "# we will use batch size of 128 in Stochastic Gradient Descent (SGD) optimization of the network\n",
        "model_args['batch_size'] = 128 \n",
        "# learning rate is how fast it will descend\n",
        "model_args['lr'] = .05\n",
        "# SGD momentum (default: .5) momentum is a moving average of gradients (it helps to keep direction) \n",
        "model_args['momentum'] = .5\n",
        "# the number of epochs is the number of times you go through the full dataset \n",
        "model_args['epochs'] = 50\n",
        "# logging frequency\n",
        "model_args['log_interval'] = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMMMbtzvpYgR"
      },
      "source": [
        "Let's start with loading the dataset and check how it looks like"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4mwzWUvpmLq"
      },
      "source": [
        "# load the MINST dataset \n",
        "transform=transforms.Compose([transforms.ToTensor(), \n",
        "                              transforms.Normalize((0.1307,), (0.3081,))])\n",
        "mnist_train = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
        "# we divide this data into training and validation subsets\n",
        "train_subset, validation_subset = torch.utils.data.random_split(mnist_train, [50000, 10000])\n",
        "test_subset = datasets.MNIST('../data', train=False, download=True, transform=transform)\n",
        "\n",
        "# define dataloaders\n",
        "loader_kwargs = {'batch_size': model_args['batch_size'], \n",
        "                 'num_workers': 8, \n",
        "                 'pin_memory': True, \n",
        "                 'shuffle': True}\n",
        "train_loader = torch.utils.data.DataLoader(train_subset, **loader_kwargs)\n",
        "validation_loader = torch.utils.data.DataLoader(validation_subset, **loader_kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(test_subset, **loader_kwargs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Bhqwulj1nfy"
      },
      "source": [
        "print(len(train_subset))\n",
        "print(len(validation_subset))\n",
        "print(len(test_subset)) # we have data already divided into train & validation & test subsets\n",
        "print(len(train_loader)) # no of train batches\n",
        "example_number = 123\n",
        "print(train_subset[example_number][0][0].shape) # single item shape\n",
        "\n",
        "fig, axs = plt.subplots(5, 5, figsize=(7,7), tight_layout=True)\n",
        "for i in range(5):\n",
        "    for j in range(5):\n",
        "        axs[i,j].imshow(train_subset[example_number+i*5+j][0].reshape(28,28), cmap='gray')\n",
        "        axs[i,j].set_title(train_subset[example_number+i*5+j][1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBkkgV5G7Bvb"
      },
      "source": [
        "Let's now construct the NN models:\n",
        "- we start with siplest single layer **perceptron** with *softmax* activation,\n",
        "- then test **deep** network with some (fully-connected) *hidden* layers,\n",
        "- and finally setup deep **convolutional neural network**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6W19lU8aLvuG"
      },
      "source": [
        "class Perceptron(nn.Module):\n",
        "    # this defines the structure of the Perceptron model\n",
        "    def __init__(self):\n",
        "        super(Perceptron, self).__init__()\n",
        "        # fully connected layer\n",
        "        self.fc = nn.Linear(28*28, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = self.fc(x)\n",
        "        return F.log_softmax(x, dim=1) # note that dim=0 is the number of samples in batch\n",
        "\n",
        "class Deep(nn.Module):\n",
        "    # this defines the structure of the Perceptron model\n",
        "    def __init__(self):\n",
        "        super(Deep, self).__init__()\n",
        "        # fully connected layers\n",
        "        self.fc1 = nn.Linear(28*28, 20)\n",
        "        self.fc2 = nn.Linear(20, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        # hidden layer\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1) # note that dim=0 is the number of samples in batch\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    # this defines the structure of the CNN model\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        # convolutional layer with 2 kernels of size 5x5\n",
        "        self.conv1 = nn.Conv2d(1, 2, kernel_size=5) \n",
        "        # 4 kernels of size 5x5\n",
        "        self.conv2 = nn.Conv2d(2, 4, kernel_size=5)\n",
        "        # 2D dropout\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        # fully connected layers\n",
        "        self.fc1 = nn.Linear(64, 20)\n",
        "        self.fc2 = nn.Linear(20, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1st layers group\n",
        "        x = self.conv1(x) # resulting in 4 feature maps each of size 24x24\n",
        "        x = F.max_pool2d(x, 2) # downsizing each map to 12x12\n",
        "        x = F.relu(x) # standard (in CNNs) ReLU activation\n",
        "        # 2nd group\n",
        "        x = self.conv2(x) # resulting in 18 feature maps each of size 8x8\n",
        "        # x = self.conv2_drop(x) \n",
        "        x = F.max_pool2d(x, 2) # downsizing each map to 4x4\n",
        "        x = F.relu(x)\n",
        "        # fully connected layers\n",
        "        x = x.view(-1, 64) # 4 maps of 4x4 size gives 64 numbers\n",
        "        x = self.fc1(x) # 64 -> 20\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, training=self.training) # dropout is a type of regularization\n",
        "        x = self.fc2(x) # 20 -> 10\n",
        "        # softmax (multinomial classification) gives probabilities of each class\n",
        "        return F.log_softmax(x, dim=1) # note that dim=0 is the number of samples in batch\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi-uXgK23kd0"
      },
      "source": [
        "And define training, testing, and plotting utils:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PzkUhvA3dg3"
      },
      "source": [
        "def train(model, device, train_loader, optimizer, epoch_number):\n",
        "    model.train()\n",
        "    train_loss = 0.\n",
        "    # get subsequent batches over the data in a given epoch\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        # send data tensors to GPU (or CPU)\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        # this will zero out the gradients for this batch\n",
        "        optimizer.zero_grad()\n",
        "        # this will execute the forward() function\n",
        "        output = model(data)\n",
        "        # calculate the negative-log-likelihood loss\n",
        "        loss = F.nll_loss(output, target, reduction='mean')\n",
        "        # backpropagate the loss\n",
        "        loss.backward()\n",
        "        # update the model weights (with assumed learning rate)\n",
        "        optimizer.step()\n",
        "        if batch_idx % model_args['log_interval'] == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch_number, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "        train_loss += loss.item()\n",
        "    train_loss /= len(train_loader)\n",
        "    print('\\nTrain set: Average loss: {:.4f}'.format(train_loss))\n",
        "    return train_loss\n",
        "    \n",
        "def test(model, device, test_loader, message):\n",
        "    model.eval()\n",
        "    test_loss = 0.\n",
        "    correct = 0\n",
        "    # this is just inference, we don't need to calculate gradients\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device) \n",
        "            output = model(data)\n",
        "            # calculate and sum up batch loss\n",
        "            test_loss += F.nll_loss(output, target, reduction='mean') \n",
        "            # get the index of class with the max log-probability \n",
        "            prediction = output.argmax(dim=1)  \n",
        "            # item() returns value of the given tensor\n",
        "            correct += prediction.eq(target).sum().item()\n",
        "    test_loss /= len(test_loader)\n",
        "    print('{}: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        message, test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    return test_loss\n",
        "\n",
        "def plot_loss(train_loss, validation_loss, title):\n",
        "    plt.grid(True)\n",
        "    plt.xlabel(\"subsequent epochs\")\n",
        "    plt.ylabel('average loss')\n",
        "    plt.plot(range(1, len(train_loss)+1), train_loss, 'o-', label='training')\n",
        "    plt.plot(range(1, len(validation_loss)+1), validation_loss, 'o-', label='validation')\n",
        "    plt.legend()\n",
        "    plt.title(title)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUMFAqUys5b0"
      },
      "source": [
        "## Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOpC0udlM-pG"
      },
      "source": [
        "1. At first let's test the single-layer *Perceptron* model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzzZ2T5FmqYe"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = Perceptron().to(device)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=model_args['lr'], momentum=model_args['momentum'])\n",
        "#optimizer = optim.Adadelta(model.parameters(), lr=model_args['lr'])\n",
        "\n",
        "torch.manual_seed(model_args['seed'])\n",
        "train_loss = []\n",
        "validation_loss = []\n",
        "for epoch_number in range(1, model_args['epochs'] + 1):\n",
        "    train_loss.append(train(model, device, train_loader, optimizer, epoch_number))\n",
        "    validation_loss.append(test(model, device, validation_loader, 'Validation set'))\n",
        "\n",
        "test(model, device, test_loader, 'Test set')\n",
        "plot_loss(train_loss, validation_loss, 'Perceptron model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFKbdciHt8Rs"
      },
      "source": [
        "2. Now add a hidden layer to the *Perceptron* and evaluate the *Deep* model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uA5QZX2dteYo"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = Deep().to(device)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=model_args['lr'], momentum=model_args['momentum'])\n",
        "#optimizer = optim.Adadelta(model.parameters(), lr=model_args['lr'])\n",
        "\n",
        "torch.manual_seed(model_args['seed'])\n",
        "train_loss = []\n",
        "validation_loss = []\n",
        "for epoch_number in range(1, model_args['epochs'] + 1):\n",
        "    train_loss.append(train(model, device, train_loader, optimizer, epoch_number))\n",
        "    validation_loss.append(test(model, device, validation_loader, 'Validation set'))\n",
        "\n",
        "test(model, device, test_loader, 'Test set')\n",
        "plot_loss(train_loss, validation_loss, 'Deep model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbu9CJdViCgJ"
      },
      "source": [
        "3. Compare these results with a deep convolutional network, which is more useful in real-world problems:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zP8rSSWkh4Qy"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = CNN().to(device)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=model_args['lr'], momentum=model_args['momentum'])\n",
        "#optimizer = optim.Adadelta(model.parameters(), lr=model_args['lr'])\n",
        "\n",
        "torch.manual_seed(model_args['seed'])\n",
        "train_loss = []\n",
        "validation_loss = []\n",
        "for epoch_number in range(1, model_args['epochs'] + 1):\n",
        "    train_loss.append(train(model, device, train_loader, optimizer, epoch_number))\n",
        "    validation_loss.append(test(model, device, validation_loader, 'Validation set'))\n",
        "\n",
        "test(model, device, test_loader, 'Test set')\n",
        "plot_loss(train_loss, validation_loss, 'CNN model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAWJMRgllBLr"
      },
      "source": [
        "## Summary\n",
        "* the two smallest (and simplest) models appear to be overfitted; to deal with this we need to apply some regularization,\n",
        "* validation dataset is evaluated during the training (as opposed to test dataset, infered *after* the training) in order to tune the model hiperparameters (that describe e.g. the network structure, or training details),\n",
        "* moment to stop trainning is a kind of hipermarameter -- by applying *early stopping* we can avoid overfitting.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mL2IBnmiZgr-"
      },
      "source": [
        "## Tasks to do\n",
        "* apply some regularization technique to *Deep* model in order to avoid overfitting,\n",
        "* try to figure out why the validation loss for *CNN* model turns to be *lower* than the train loss (hint: turn off regularization),\n",
        "* **tune one of these models to get the *Test Set Accuracy* > 99%**,\n",
        "* plot the *confusion matrix* among all of the classes---which of digits are mostly confused with each other?,\n",
        "* and finally prepare report.\n",
        "\n",
        "### How the report should look like\n",
        "* 1-2 page long\n",
        "* shortly describe what modifications you apply\n",
        "* justyfy in a few words why your improved model is so cool\n",
        "* results should be presented as plots with multiline captions"
      ]
    }
  ]
}